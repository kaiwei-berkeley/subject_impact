{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import os\n",
    "import math\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataframe that matches paper abstract with subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_id = []\n",
    "abstract = []\n",
    "\n",
    "f = open('aminer_2013.txt','r',encoding = 'utf8')\n",
    "f.readline()\n",
    "for i, line in enumerate(f):\n",
    "        if (i+2) % 250000 == 0:\n",
    "            print('file '+file+': ',round((i+2)/1000000*100,1),\"%\")\n",
    "        json_line = json.loads(line)\n",
    "        if 'year' in json_line and 'keywords' in json_line and \\\n",
    "        'abstract' in json_line and 'lang' in json_line and \\\n",
    "        'references' in json_line and 'issn' in json_line:\n",
    "            \n",
    "            if json_line['lang'] == 'en' :\n",
    "                \n",
    "                ## store paper info, later use to get the subject of the paper\n",
    "                paper_id.append(json_line['id'])\n",
    "                abstract.append(json_line['abstract'])\n",
    "\n",
    "                \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53e99796b7602d9701f5c805</th>\n",
       "      <td>Scientific publishing has become synonymous wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53e99796b7602d9701f5e179</th>\n",
       "      <td>Summary We present a model of succession in a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53e997a2b7602d9701f720e9</th>\n",
       "      <td>The U.S. patent system is overdue for reform, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53e997aab7602d9701f82f8c</th>\n",
       "      <td>Professor Barabási's talk described how the to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53e997bdb7602d9701fab322</th>\n",
       "      <td>We characterize ``visual textures'' as realiza...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53e997c6b7602d9701fb400a</th>\n",
       "      <td>Brainbow is a genetic engineering technique th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53e997c6b7602d9701fb8dab</th>\n",
       "      <td>Niels Bohr's (1885–1962) ► atomic model initia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53e997cbb7602d9701fbb959</th>\n",
       "      <td>Patient safety is a global challenge that requ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53e997d1b7602d9701fc73fd</th>\n",
       "      <td>Di- and tri-phosphate nucleotides are essentia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53e997d7b7602d9701fcc002</th>\n",
       "      <td>This paper presents the design and optimizatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53e997d7b7602d9701fccdd2</th>\n",
       "      <td>In order to overcome the problems of high comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53e997ddb7602d9701fd727a</th>\n",
       "      <td>To establish 3D models of coronary arteries (C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53e997e4b7602d9701fdaff4</th>\n",
       "      <td>Even though 3D flash memory presents a grand o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53e997e4b7602d9701fdb43a</th>\n",
       "      <td>We propose a new analytical method for detecti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53e997e8b7602d9701fdfeec</th>\n",
       "      <td>Given a set B of n black points in general pos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53e997e8b7602d9701fe0159</th>\n",
       "      <td>In biomedical research a growing number of pla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53e997e8b7602d9701fe1a23</th>\n",
       "      <td>Cytoscape is an open source software tool for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53e997ecb7602d9701fea9bb</th>\n",
       "      <td>It is pointed out that in an economy with only...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53e997edb7602d9701fed24b</th>\n",
       "      <td>Large genomic databases with interactive acces...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53e997f1b7602d9701fee7df</th>\n",
       "      <td>This paper introduces a novel classification m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53e997f4b7602d9701ff55f7</th>\n",
       "      <td>Biomedical corpora annotated with event-level ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53e997f8b7602d9701ffe44f</th>\n",
       "      <td>Health information provides the foundation for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53e997fcb7602d9702009fa6</th>\n",
       "      <td>Summary:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53e99800b7602d970200b330</th>\n",
       "      <td>Tagging items with descriptive annotations or ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53e99803b7602d9702013852</th>\n",
       "      <td>Crowdsourcing is based on a simple but powerfu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53e99804b7602d9702018427</th>\n",
       "      <td>We address the problem of estimating the diffe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53e99808b7602d9702019db5</th>\n",
       "      <td>The suffix -ome conveys \"comprehensiveness\" in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53e99809b7602d970201f557</th>\n",
       "      <td>Neurophysiologically, central apnea is due to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53e99809b7602d970201fd94</th>\n",
       "      <td>Adenocarcinomas of the colon and rectum (color...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53e99809b7602d9702021054</th>\n",
       "      <td>Prostate cancer is one of the leading causes o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53e9a059b7602d9702940967</th>\n",
       "      <td>POMDP is considered as a basic model for decis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53e9a059b7602d9702940ba1</th>\n",
       "      <td>: We propose three algorithms for string edit ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53e9a059b7602d9702941148</th>\n",
       "      <td>The brain must dynamically integrate, coordina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53e9a05fb7602d970294243f</th>\n",
       "      <td>Multistatic active sonar systems involve the t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53e9a05fb7602d97029443f3</th>\n",
       "      <td>The moral force of impartiality (i.e. the equa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53e9a060b7602d9702946226</th>\n",
       "      <td>A variety of methods have been developed for v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53e9a060b7602d9702947549</th>\n",
       "      <td>A tandem technique of hard equipment is often ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53e9a060b7602d9702947748</th>\n",
       "      <td>Internal coordinate molecular dynamics (ICMD) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53e9a060b7602d9702947c00</th>\n",
       "      <td>Dynamic optimization problems (DOPs) are those...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53e9a066b7602d970294a671</th>\n",
       "      <td>ReRAM has been considered as one of the potent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53e9a066b7602d970294c511</th>\n",
       "      <td>Radar interferometry has been widely applied i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53e9a066b7602d970294cb5f</th>\n",
       "      <td>The stabilization of exponentially unstable li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53e9a067b7602d970294fcd9</th>\n",
       "      <td>We develop a simple yet realistic model of inc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53e9a06cb7602d9702951666</th>\n",
       "      <td>Filopodia are small cellular projections that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53e9a06db7602d9702954640</th>\n",
       "      <td>Major Histocompatibility Complex (MHC), peptid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53e9a06db7602d9702954c93</th>\n",
       "      <td>Context has been playing an increasingly impor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53e9a073b7602d97029595c8</th>\n",
       "      <td>An effective hybrid evolutionary search method...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53e9a073b7602d970295accf</th>\n",
       "      <td>Plastids are an important component of plant c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53e9a073b7602d970295ae14</th>\n",
       "      <td>The HLA (human leukocyte antigen) class I is a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53e9a073b7602d970295b670</th>\n",
       "      <td>Little is known about the potential of syndrom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53e9a079b7602d970295f7b4</th>\n",
       "      <td>With the increased use of ontologies and ontol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53e9a079b7602d970295fde8</th>\n",
       "      <td>Positron emission tomography (PET) scanner wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53e9a07ab7602d9702961479</th>\n",
       "      <td>The estimation of isoform abundances from RNA-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53e9a07ab7602d97029629f8</th>\n",
       "      <td>The second generation of the BeiDou Navigation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53e9a07ab7602d97029645aa</th>\n",
       "      <td>The purpose of this study was to examine alter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53e9a07ab7602d9702964ca8</th>\n",
       "      <td>In this letter, we present an original demonst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53e9a081b7602d970296709a</th>\n",
       "      <td>The Third Generation Partnership Project (3GPP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53e9a082b7602d9702969865</th>\n",
       "      <td>We describe a case of heterotopic ossification...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53e9a082b7602d970296a62d</th>\n",
       "      <td>The genetic and neural basis of working memory...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53e9a089b7602d970296ef15</th>\n",
       "      <td>After more than a decade since microarrays wer...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>242322 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                   abstract\n",
       "id                                                                         \n",
       "53e99796b7602d9701f5c805  Scientific publishing has become synonymous wi...\n",
       "53e99796b7602d9701f5e179  Summary We present a model of succession in a ...\n",
       "53e997a2b7602d9701f720e9  The U.S. patent system is overdue for reform, ...\n",
       "53e997aab7602d9701f82f8c  Professor Barabási's talk described how the to...\n",
       "53e997bdb7602d9701fab322  We characterize ``visual textures'' as realiza...\n",
       "53e997c6b7602d9701fb400a  Brainbow is a genetic engineering technique th...\n",
       "53e997c6b7602d9701fb8dab  Niels Bohr's (1885–1962) ► atomic model initia...\n",
       "53e997cbb7602d9701fbb959  Patient safety is a global challenge that requ...\n",
       "53e997d1b7602d9701fc73fd  Di- and tri-phosphate nucleotides are essentia...\n",
       "53e997d7b7602d9701fcc002  This paper presents the design and optimizatio...\n",
       "53e997d7b7602d9701fccdd2  In order to overcome the problems of high comp...\n",
       "53e997ddb7602d9701fd727a  To establish 3D models of coronary arteries (C...\n",
       "53e997e4b7602d9701fdaff4  Even though 3D flash memory presents a grand o...\n",
       "53e997e4b7602d9701fdb43a  We propose a new analytical method for detecti...\n",
       "53e997e8b7602d9701fdfeec  Given a set B of n black points in general pos...\n",
       "53e997e8b7602d9701fe0159  In biomedical research a growing number of pla...\n",
       "53e997e8b7602d9701fe1a23  Cytoscape is an open source software tool for ...\n",
       "53e997ecb7602d9701fea9bb  It is pointed out that in an economy with only...\n",
       "53e997edb7602d9701fed24b  Large genomic databases with interactive acces...\n",
       "53e997f1b7602d9701fee7df  This paper introduces a novel classification m...\n",
       "53e997f4b7602d9701ff55f7  Biomedical corpora annotated with event-level ...\n",
       "53e997f8b7602d9701ffe44f  Health information provides the foundation for...\n",
       "53e997fcb7602d9702009fa6                                           Summary:\n",
       "53e99800b7602d970200b330  Tagging items with descriptive annotations or ...\n",
       "53e99803b7602d9702013852  Crowdsourcing is based on a simple but powerfu...\n",
       "53e99804b7602d9702018427  We address the problem of estimating the diffe...\n",
       "53e99808b7602d9702019db5  The suffix -ome conveys \"comprehensiveness\" in...\n",
       "53e99809b7602d970201f557  Neurophysiologically, central apnea is due to ...\n",
       "53e99809b7602d970201fd94  Adenocarcinomas of the colon and rectum (color...\n",
       "53e99809b7602d9702021054  Prostate cancer is one of the leading causes o...\n",
       "...                                                                     ...\n",
       "53e9a059b7602d9702940967  POMDP is considered as a basic model for decis...\n",
       "53e9a059b7602d9702940ba1  : We propose three algorithms for string edit ...\n",
       "53e9a059b7602d9702941148  The brain must dynamically integrate, coordina...\n",
       "53e9a05fb7602d970294243f  Multistatic active sonar systems involve the t...\n",
       "53e9a05fb7602d97029443f3  The moral force of impartiality (i.e. the equa...\n",
       "53e9a060b7602d9702946226  A variety of methods have been developed for v...\n",
       "53e9a060b7602d9702947549  A tandem technique of hard equipment is often ...\n",
       "53e9a060b7602d9702947748  Internal coordinate molecular dynamics (ICMD) ...\n",
       "53e9a060b7602d9702947c00  Dynamic optimization problems (DOPs) are those...\n",
       "53e9a066b7602d970294a671  ReRAM has been considered as one of the potent...\n",
       "53e9a066b7602d970294c511  Radar interferometry has been widely applied i...\n",
       "53e9a066b7602d970294cb5f  The stabilization of exponentially unstable li...\n",
       "53e9a067b7602d970294fcd9  We develop a simple yet realistic model of inc...\n",
       "53e9a06cb7602d9702951666  Filopodia are small cellular projections that ...\n",
       "53e9a06db7602d9702954640  Major Histocompatibility Complex (MHC), peptid...\n",
       "53e9a06db7602d9702954c93  Context has been playing an increasingly impor...\n",
       "53e9a073b7602d97029595c8  An effective hybrid evolutionary search method...\n",
       "53e9a073b7602d970295accf  Plastids are an important component of plant c...\n",
       "53e9a073b7602d970295ae14  The HLA (human leukocyte antigen) class I is a...\n",
       "53e9a073b7602d970295b670  Little is known about the potential of syndrom...\n",
       "53e9a079b7602d970295f7b4  With the increased use of ontologies and ontol...\n",
       "53e9a079b7602d970295fde8  Positron emission tomography (PET) scanner wit...\n",
       "53e9a07ab7602d9702961479  The estimation of isoform abundances from RNA-...\n",
       "53e9a07ab7602d97029629f8  The second generation of the BeiDou Navigation...\n",
       "53e9a07ab7602d97029645aa  The purpose of this study was to examine alter...\n",
       "53e9a07ab7602d9702964ca8  In this letter, we present an original demonst...\n",
       "53e9a081b7602d970296709a  The Third Generation Partnership Project (3GPP...\n",
       "53e9a082b7602d9702969865  We describe a case of heterotopic ossification...\n",
       "53e9a082b7602d970296a62d  The genetic and neural basis of working memory...\n",
       "53e9a089b7602d970296ef15  After more than a decade since microarrays wer...\n",
       "\n",
       "[242322 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.DataFrame()\n",
    "df['id'] = paper_id\n",
    "df['abstract'] = abstract\n",
    "df.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53e99796b7602d9701f5c805</td>\n",
       "      <td>Scientific publishing has become synonymous wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53e99796b7602d9701f5e179</td>\n",
       "      <td>Summary We present a model of succession in a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53e997a2b7602d9701f720e9</td>\n",
       "      <td>The U.S. patent system is overdue for reform, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53e997aab7602d9701f82f8c</td>\n",
       "      <td>Professor Barabási's talk described how the to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>53e997bdb7602d9701fab322</td>\n",
       "      <td>We characterize ``visual textures'' as realiza...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id                                           abstract\n",
       "0  53e99796b7602d9701f5c805  Scientific publishing has become synonymous wi...\n",
       "1  53e99796b7602d9701f5e179  Summary We present a model of succession in a ...\n",
       "2  53e997a2b7602d9701f720e9  The U.S. patent system is overdue for reform, ...\n",
       "3  53e997aab7602d9701f82f8c  Professor Barabási's talk described how the to...\n",
       "4  53e997bdb7602d9701fab322  We characterize ``visual textures'' as realiza..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = pd.read_csv('paper_subject_match_subfield.csv',index_col = 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm = pd.merge(df, subject, on = ['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          0\n",
       "abstract    0\n",
       "subfield    0\n",
       "year        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "242322"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm = tm.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm = tm.drop(columns = ['id', 'year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>subfield</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Scientific publishing has become synonymous wi...</td>\n",
       "      <td>1307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Summary We present a model of succession in a ...</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The U.S. patent system is overdue for reform, ...</td>\n",
       "      <td>2719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Professor Barabási's talk described how the to...</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We characterize ``visual textures'' as realiza...</td>\n",
       "      <td>1705</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            abstract  subfield\n",
       "0  Scientific publishing has become synonymous wi...      1307\n",
       "1  Summary We present a model of succession in a ...      1000\n",
       "2  The U.S. patent system is overdue for reform, ...      2719\n",
       "3  Professor Barabási's talk described how the to...      1000\n",
       "4  We characterize ``visual textures'' as realiza...      1705"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm['subfield'] = tm['subfield'].apply(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "292"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tm.subfield.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>subfield</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Scientific publishing has become synonymous wi...</td>\n",
       "      <td>1307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Summary We present a model of succession in a ...</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The U.S. patent system is overdue for reform, ...</td>\n",
       "      <td>2719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Professor Barabási's talk described how the to...</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We characterize ``visual textures'' as realiza...</td>\n",
       "      <td>1705</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            abstract  subfield\n",
       "0  Scientific publishing has become synonymous wi...      1307\n",
       "1  Summary We present a model of succession in a ...      1000\n",
       "2  The U.S. patent system is overdue for reform, ...      2719\n",
       "3  Professor Barabási's talk described how the to...      1000\n",
       "4  We characterize ``visual textures'' as realiza...      1705"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tm.groupby('subfield')['abstract'].apply(list).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for text preprocessing\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "#nltk.download('wordnet') \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "##Creating a list of stop words and adding custom stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "##Creating a list of custom stopwords\n",
    "new_words = [\"using\", \"show\", \"result\", \"large\", \"also\", \"iv\", \"one\", \"two\", \"new\", \"previously\", \"shown\"]\n",
    "stop_words = stop_words.union(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!{sys.executable} -m pip install gensim\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "np.random.seed(2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v')) #lemmatize as verb, default is noun\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix\n",
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "#Function for sorting tf_idf in descending order\n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    " \n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    \n",
    "    # word index and corresponding tf-idf score\n",
    "    for idx, score in sorted_items:\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    " \n",
    "    #create a tuples of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = {}\n",
    "for i in df.keys():\n",
    "    for j in range(len(df[i])):\n",
    "        processed_docs.setdefault(i, []).append(preprocess(df[i][j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_take = {}\n",
    "for i in processed_docs.keys():\n",
    "    for j in range(len(processed_docs[i])):\n",
    "        df_take.setdefault(i, []).append(' '.join(processed_docs[i][j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## do not run\n",
    "#df_new = {}\n",
    "#for i in processed_docs.keys():\n",
    "    #for j in range(int(len(processed_docs[i])/2)):\n",
    "        #df_new.setdefault(i, []).append(processed_docs[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_result = []\n",
    "index_save = []\n",
    "for i in df_take.keys():\n",
    "    index_save.append(i)\n",
    "    \n",
    "for index, i in enumerate(df_take): \n",
    "    df_copy = df_take.copy()\n",
    "    ex_index = [x for x in index_save if x !=i]\n",
    "    for j in ex_index:\n",
    "        if len(df_copy[j]) > 10:\n",
    "            df_copy.update({j: random.sample(df_copy[j], 10)})\n",
    "    \n",
    "    if len(df_copy[i]) > 150:\n",
    "        df_copy.update({i: random.sample(df_copy[i], 150)})\n",
    "    \n",
    "    for j in df_copy.keys():\n",
    "        df_copy.update({j: ' '.join(df_copy[j])}) \n",
    "    \n",
    "    \n",
    "    corpus = []\n",
    "    for j in df_copy.keys():\n",
    "        corpus.append(df_copy[j])\n",
    "    \n",
    "    cv=CountVectorizer(max_df=0.8,stop_words=stop_words, max_features=10000)\n",
    "    X=cv.fit_transform(corpus)\n",
    "\n",
    "\n",
    "    tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "    tfidf_transformer.fit(X)\n",
    "    # get feature names\n",
    "    feature_names=cv.get_feature_names()\n",
    "\n",
    "    # fetch document for which keywords needs to be extracted\n",
    "    doc=corpus[index]\n",
    "\n",
    "    #generate tf-idf for the given document\n",
    "    tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))\n",
    "\n",
    "    #sort the tf-idf vectors by descending order of scores\n",
    "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "    #extract only the top n; n here is 10\n",
    "    keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n",
    "\n",
    "    new_result.append([e for e in keywords.keys()])\n",
    "    \n",
    "\n",
    "    sorted_items.clear()\n",
    "    keywords.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*********# SONG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_word_list = [item for sublist in new_result for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_copy = processed_docs.copy()\n",
    "for key in [key for key in df if len(proc_copy[key]) < 30]: del proc_copy[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "import random\n",
    "random.seed(2)\n",
    "for i in proc_copy.keys():\n",
    "    l = random.sample(range(len(proc_copy[i])), 30)\n",
    "    for x in l:\n",
    "        train.append(proc_copy[i][x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_doc = []\n",
    "for i in range(len(train)):\n",
    "    new_doc.append([k for k in train[i] if k in all_word_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "def LDA_TF_word(doc):\n",
    "    dictionary = gensim.corpora.Dictionary(doc) \n",
    "    bow_corpus = [dictionary.doc2bow(d) for d in doc]\n",
    "    tfidf = models.TfidfModel(bow_corpus)\n",
    "    corpus_tfidf = tfidf[bow_corpus]\n",
    "    lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=20, id2word=dictionary, passes=2, workers=4,minimum_probability=0)\n",
    "    return(lda_model_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = new_doc.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_model = LDA_TF_word(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.008*\"activ\" + 0.008*\"patient\" + 0.005*\"risk\" + 0.005*\"data\" + 0.005*\"structur\" + 0.005*\"interact\" + 0.005*\"test\" + 0.005*\"level\" + 0.005*\"method\" + 0.005*\"treatment\"\n",
      "Topic: 1 Word: 0.015*\"patient\" + 0.008*\"trial\" + 0.007*\"group\" + 0.007*\"clinic\" + 0.006*\"control\" + 0.006*\"sequenc\" + 0.006*\"treatment\" + 0.006*\"nerv\" + 0.005*\"cell\" + 0.005*\"intervent\"\n",
      "Topic: 2 Word: 0.013*\"cell\" + 0.008*\"activ\" + 0.006*\"imag\" + 0.006*\"protein\" + 0.006*\"model\" + 0.005*\"patient\" + 0.005*\"express\" + 0.005*\"treatment\" + 0.005*\"compound\" + 0.005*\"speci\"\n",
      "Topic: 3 Word: 0.007*\"group\" + 0.006*\"health\" + 0.006*\"method\" + 0.006*\"factor\" + 0.006*\"data\" + 0.006*\"intervent\" + 0.005*\"problem\" + 0.005*\"children\" + 0.005*\"mental\" + 0.005*\"nurs\"\n",
      "Topic: 4 Word: 0.017*\"patient\" + 0.009*\"clinic\" + 0.007*\"care\" + 0.007*\"temperatur\" + 0.007*\"hospit\" + 0.006*\"sexual\" + 0.006*\"treatment\" + 0.006*\"injuri\" + 0.006*\"trauma\" + 0.006*\"cancer\"\n",
      "Topic: 5 Word: 0.013*\"patient\" + 0.009*\"diabet\" + 0.008*\"level\" + 0.008*\"children\" + 0.007*\"sleep\" + 0.007*\"group\" + 0.006*\"model\" + 0.006*\"measur\" + 0.005*\"predict\" + 0.005*\"associ\"\n",
      "Topic: 6 Word: 0.008*\"water\" + 0.006*\"method\" + 0.006*\"model\" + 0.006*\"film\" + 0.006*\"electrod\" + 0.006*\"group\" + 0.005*\"sampl\" + 0.005*\"milk\" + 0.005*\"infect\" + 0.005*\"measur\"\n",
      "Topic: 7 Word: 0.021*\"cell\" + 0.009*\"activ\" + 0.008*\"protein\" + 0.008*\"express\" + 0.007*\"induc\" + 0.007*\"patient\" + 0.006*\"human\" + 0.006*\"inhibit\" + 0.005*\"treatment\" + 0.005*\"gene\"\n",
      "Topic: 8 Word: 0.009*\"surfac\" + 0.007*\"measur\" + 0.007*\"model\" + 0.006*\"imag\" + 0.006*\"resist\" + 0.006*\"flow\" + 0.006*\"method\" + 0.006*\"data\" + 0.005*\"group\" + 0.005*\"error\"\n",
      "Topic: 9 Word: 0.008*\"protein\" + 0.008*\"model\" + 0.008*\"group\" + 0.008*\"cell\" + 0.007*\"complex\" + 0.007*\"activ\" + 0.007*\"reaction\" + 0.006*\"bone\" + 0.006*\"product\" + 0.006*\"electron\"\n",
      "Topic: 10 Word: 0.011*\"bone\" + 0.009*\"patient\" + 0.007*\"cell\" + 0.006*\"treatment\" + 0.006*\"disast\" + 0.006*\"level\" + 0.006*\"miner\" + 0.005*\"injuri\" + 0.005*\"group\" + 0.005*\"virus\"\n",
      "Topic: 11 Word: 0.018*\"patient\" + 0.016*\"cancer\" + 0.008*\"treatment\" + 0.007*\"tumor\" + 0.007*\"group\" + 0.007*\"year\" + 0.006*\"diseas\" + 0.006*\"clinic\" + 0.006*\"children\" + 0.006*\"risk\"\n",
      "Topic: 12 Word: 0.007*\"patient\" + 0.006*\"method\" + 0.006*\"acid\" + 0.006*\"gene\" + 0.005*\"famili\" + 0.005*\"activ\" + 0.005*\"group\" + 0.005*\"control\" + 0.005*\"cell\" + 0.005*\"propos\"\n",
      "Topic: 13 Word: 0.009*\"soil\" + 0.007*\"nanoparticl\" + 0.007*\"model\" + 0.007*\"optim\" + 0.007*\"function\" + 0.006*\"network\" + 0.006*\"method\" + 0.005*\"estim\" + 0.005*\"patient\" + 0.005*\"cell\"\n",
      "Topic: 14 Word: 0.010*\"cell\" + 0.007*\"model\" + 0.007*\"sperm\" + 0.007*\"patient\" + 0.006*\"express\" + 0.006*\"imag\" + 0.006*\"process\" + 0.006*\"method\" + 0.005*\"propos\" + 0.005*\"measur\"\n",
      "Topic: 15 Word: 0.007*\"cell\" + 0.007*\"method\" + 0.006*\"propos\" + 0.006*\"model\" + 0.006*\"estim\" + 0.005*\"group\" + 0.005*\"equat\" + 0.005*\"temperatur\" + 0.005*\"problem\" + 0.005*\"paper\"\n",
      "Topic: 16 Word: 0.012*\"patient\" + 0.009*\"women\" + 0.007*\"cardiac\" + 0.006*\"group\" + 0.006*\"care\" + 0.005*\"pain\" + 0.005*\"cell\" + 0.005*\"health\" + 0.005*\"year\" + 0.005*\"inform\"\n",
      "Topic: 17 Word: 0.010*\"solut\" + 0.008*\"network\" + 0.008*\"model\" + 0.007*\"problem\" + 0.007*\"paper\" + 0.007*\"equat\" + 0.007*\"method\" + 0.006*\"propos\" + 0.006*\"activ\" + 0.006*\"group\"\n",
      "Topic: 18 Word: 0.010*\"patient\" + 0.008*\"diabet\" + 0.007*\"insulin\" + 0.007*\"associ\" + 0.007*\"model\" + 0.007*\"stress\" + 0.006*\"risk\" + 0.006*\"express\" + 0.006*\"group\" + 0.006*\"activ\"\n",
      "Topic: 19 Word: 0.010*\"patient\" + 0.009*\"cell\" + 0.008*\"nurs\" + 0.006*\"servic\" + 0.006*\"model\" + 0.005*\"spinal\" + 0.005*\"propos\" + 0.005*\"data\" + 0.005*\"scheme\" + 0.005*\"group\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in rep_model.print_topics(-1):\n",
    "        print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_copy = processed_docs.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropped if less than 30\n",
    "for key in [key for key in df if len(pd_copy[key]) < 30]: del pd_copy[key] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample 30 from each subfield\n",
    "sample_bysub = {}\n",
    "import random\n",
    "random.seed(2014)\n",
    "for i in pd_copy.keys():\n",
    "    ls = random.sample(range(len(pd_copy[i])), 30)\n",
    "    for x in ls:\n",
    "        sample_bysub.setdefault(i, []).append(pd_copy[i][x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([1000, 1100, 1102, 1103, 1104, 1105, 1106, 1108, 1109, 1110, 1111, 1201, 1300, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1500, 1502, 1503, 1505, 1600, 1602, 1603, 1604, 1605, 1606, 1607, 1700, 1702, 1704, 1705, 1706, 1707, 1708, 1710, 1711, 1712, 1803, 1900, 1902, 1906, 1907, 1910, 1912, 2002, 2103, 2200, 2201, 2204, 2205, 2207, 2208, 2209, 2210, 2211, 2213, 2300, 2303, 2304, 2305, 2306, 2307, 2308, 2310, 2312, 2400, 2401, 2402, 2403, 2404, 2405, 2406, 2500, 2502, 2504, 2505, 2507, 2508, 2600, 2601, 2602, 2603, 2604, 2605, 2606, 2607, 2610, 2611, 2613, 2700, 2701, 2702, 2703, 2704, 2705, 2706, 2707, 2708, 2710, 2711, 2712, 2713, 2714, 2715, 2716, 2717, 2718, 2719, 2720, 2721, 2722, 2723, 2724, 2725, 2726, 2727, 2728, 2729, 2730, 2731, 2732, 2733, 2734, 2735, 2736, 2737, 2738, 2739, 2740, 2741, 2742, 2743, 2745, 2746, 2747, 2748, 2800, 2802, 2803, 2804, 2805, 2806, 2807, 2808, 2809, 2900, 2902, 2907, 2909, 2910, 2911, 2913, 2916, 2919, 2921, 3000, 3002, 3003, 3004, 3005, 3100, 3101, 3102, 3104, 3105, 3106, 3107, 3108, 3200, 3202, 3203, 3204, 3205, 3206, 3207, 3301, 3304, 3305, 3306, 3307, 3308, 3312, 3314, 3317, 3400, 3402, 3404, 3500, 3504, 3505, 3506, 3602, 3609, 3612, 3614, 3616])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_bysub.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample_bysub[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_rep = gensim.corpora.Dictionary(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_models(text_str):\n",
    "        bow_vector = dictionary_rep.doc2bow(text_str)\n",
    "        sc = rep_model[bow_vector]\n",
    "        score = list()\n",
    "        for x in range(len(sc)):\n",
    "            score.append(sc[x][1])\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_score = {}\n",
    "for i in sample_bysub.keys():\n",
    "    for j in range(len(sample_bysub[i])):\n",
    "        score = feed_models(sample_bysub[i][j])\n",
    "        store_score.setdefault(i, []).append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "\n",
    "def geometric_median(points, method='auto', options={}):\n",
    "    \"\"\"\n",
    "    Calculates the geometric median of an array of points.\n",
    "    method specifies which algorithm to use:\n",
    "        * 'auto' -- uses a heuristic to pick an algorithm\n",
    "        * 'minimize' -- scipy.optimize the sum of distances\n",
    "        * 'weiszfeld' -- Weiszfeld's algorithm\n",
    "    \"\"\"\n",
    "\n",
    "    points = np.asarray(points)\n",
    "\n",
    "    if len(points.shape) == 1:\n",
    "        # geometric_median((0, 0)) has too much potential for error.\n",
    "        # Did the user intend a single 2D point or two scalars?\n",
    "        # Use np.median if you meant the latter.\n",
    "        raise ValueError(\"Expected 2D array\")\n",
    "\n",
    "    if method == 'auto':\n",
    "        if points.shape[1] > 2:\n",
    "            # weiszfeld tends to converge faster in higher dimensions\n",
    "            method = 'weiszfeld'\n",
    "        else:\n",
    "            method = 'minimize'\n",
    "\n",
    "    return _methods[method](points, options)\n",
    "\n",
    "\n",
    "def minimize_method(points, options={}):\n",
    "    \"\"\"\n",
    "    Geometric median as a convex optimization problem.\n",
    "    \"\"\"\n",
    "\n",
    "    # objective function\n",
    "    def aggregate_distance(x):\n",
    "        return cdist([x], points).sum()\n",
    "\n",
    "    # initial guess: centroid\n",
    "    centroid = points.mean(axis=0)\n",
    "\n",
    "    optimize_result = minimize(aggregate_distance, centroid, method='COBYLA')\n",
    "\n",
    "    return optimize_result.x\n",
    "\n",
    "\n",
    "def weiszfeld_method(points, options={}):\n",
    "    \"\"\"\n",
    "    Weiszfeld's algorithm as described on Wikipedia.\n",
    "    \"\"\"\n",
    "\n",
    "    default_options = {'maxiter': 1000, 'tol': 1e-7}\n",
    "    default_options.update(options)\n",
    "    options = default_options\n",
    "\n",
    "    def distance_func(x):\n",
    "        return cdist([x], points)\n",
    "\n",
    "    # initial guess: centroid\n",
    "    guess = points.mean(axis=0)\n",
    "\n",
    "    iters = 0\n",
    "\n",
    "    while iters < options['maxiter']:\n",
    "        distances = distance_func(guess).T\n",
    "\n",
    "        # catch divide by zero\n",
    "        # TODO: Wikipedia cites how to deal with distance 0\n",
    "        distances = np.where(distances == 0, 1, distances)\n",
    "\n",
    "        guess_next = (points/distances).sum(axis=0) / (1./distances).sum(axis=0)\n",
    "\n",
    "        guess_movement = np.sqrt(((guess - guess_next)**2).sum())\n",
    "\n",
    "        guess = guess_next\n",
    "\n",
    "        if guess_movement <= options['tol']:\n",
    "            break\n",
    "\n",
    "        iters += 1\n",
    "\n",
    "    return guess\n",
    "\n",
    "\n",
    "_methods = {\n",
    "    'minimize': minimize_method,\n",
    "    'weiszfeld': weiszfeld_method,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_score = {}\n",
    "for i in store_score.keys():\n",
    "    median = geometric_median(store_score[i])\n",
    "    median_score.setdefault(i, []).append(median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "dist_store = {}\n",
    "for i in median_score.keys():\n",
    "    sub_dist = {}\n",
    "    for j in median_score.keys():\n",
    "        dist = distance.euclidean(median_score[i], median_score[j])\n",
    "        sub_dist.update({j:dist})\n",
    "    dist_store.update({i:sub_dist})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_df = sorted(subject[\"subfield\"].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame()\n",
    "df1[\"sub1\"] = np.repeat(l_df,309).tolist()\n",
    "df1[\"sub2\"] = l_df*309"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = []\n",
    "for i in l_df:\n",
    "    for j in l_df:\n",
    "        if i in dist_store.keys():\n",
    "            if j in dist_store[i].keys():\n",
    "                dist.append(dist_store[i][j])\n",
    "            else:\n",
    "                dist.append(None)\n",
    "        else:\n",
    "            dist.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"distance\"] = dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sub1</th>\n",
       "      <th>sub2</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28830</th>\n",
       "      <td>2002</td>\n",
       "      <td>2002</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29012</th>\n",
       "      <td>2002</td>\n",
       "      <td>3312</td>\n",
       "      <td>0.086777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28999</th>\n",
       "      <td>2002</td>\n",
       "      <td>3207</td>\n",
       "      <td>0.129757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29001</th>\n",
       "      <td>2002</td>\n",
       "      <td>3301</td>\n",
       "      <td>0.154026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29007</th>\n",
       "      <td>2002</td>\n",
       "      <td>3307</td>\n",
       "      <td>0.179051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29005</th>\n",
       "      <td>2002</td>\n",
       "      <td>3305</td>\n",
       "      <td>0.184566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29017</th>\n",
       "      <td>2002</td>\n",
       "      <td>3317</td>\n",
       "      <td>0.205703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28850</th>\n",
       "      <td>2002</td>\n",
       "      <td>2213</td>\n",
       "      <td>0.236648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29006</th>\n",
       "      <td>2002</td>\n",
       "      <td>3306</td>\n",
       "      <td>0.238677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28996</th>\n",
       "      <td>2002</td>\n",
       "      <td>3204</td>\n",
       "      <td>0.258080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28898</th>\n",
       "      <td>2002</td>\n",
       "      <td>2701</td>\n",
       "      <td>0.265088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28934</th>\n",
       "      <td>2002</td>\n",
       "      <td>2738</td>\n",
       "      <td>0.271287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28915</th>\n",
       "      <td>2002</td>\n",
       "      <td>2719</td>\n",
       "      <td>0.272533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28971</th>\n",
       "      <td>2002</td>\n",
       "      <td>2919</td>\n",
       "      <td>0.272610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28994</th>\n",
       "      <td>2002</td>\n",
       "      <td>3202</td>\n",
       "      <td>0.276769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28846</th>\n",
       "      <td>2002</td>\n",
       "      <td>2209</td>\n",
       "      <td>0.277370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28995</th>\n",
       "      <td>2002</td>\n",
       "      <td>3203</td>\n",
       "      <td>0.279720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29043</th>\n",
       "      <td>2002</td>\n",
       "      <td>3612</td>\n",
       "      <td>0.292031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28963</th>\n",
       "      <td>2002</td>\n",
       "      <td>2910</td>\n",
       "      <td>0.299548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28806</th>\n",
       "      <td>2002</td>\n",
       "      <td>1708</td>\n",
       "      <td>0.301755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28935</th>\n",
       "      <td>2002</td>\n",
       "      <td>2739</td>\n",
       "      <td>0.304889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28998</th>\n",
       "      <td>2002</td>\n",
       "      <td>3206</td>\n",
       "      <td>0.307462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28992</th>\n",
       "      <td>2002</td>\n",
       "      <td>3200</td>\n",
       "      <td>0.307856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28954</th>\n",
       "      <td>2002</td>\n",
       "      <td>2900</td>\n",
       "      <td>0.313989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28962</th>\n",
       "      <td>2002</td>\n",
       "      <td>2909</td>\n",
       "      <td>0.314182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29014</th>\n",
       "      <td>2002</td>\n",
       "      <td>3314</td>\n",
       "      <td>0.320662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28966</th>\n",
       "      <td>2002</td>\n",
       "      <td>2913</td>\n",
       "      <td>0.328911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28997</th>\n",
       "      <td>2002</td>\n",
       "      <td>3205</td>\n",
       "      <td>0.329097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28913</th>\n",
       "      <td>2002</td>\n",
       "      <td>2717</td>\n",
       "      <td>0.330693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28964</th>\n",
       "      <td>2002</td>\n",
       "      <td>2911</td>\n",
       "      <td>0.332313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28976</th>\n",
       "      <td>2002</td>\n",
       "      <td>3001</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28984</th>\n",
       "      <td>2002</td>\n",
       "      <td>3103</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28990</th>\n",
       "      <td>2002</td>\n",
       "      <td>3109</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28991</th>\n",
       "      <td>2002</td>\n",
       "      <td>3110</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28993</th>\n",
       "      <td>2002</td>\n",
       "      <td>3201</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29000</th>\n",
       "      <td>2002</td>\n",
       "      <td>3300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29002</th>\n",
       "      <td>2002</td>\n",
       "      <td>3302</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29003</th>\n",
       "      <td>2002</td>\n",
       "      <td>3303</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29009</th>\n",
       "      <td>2002</td>\n",
       "      <td>3309</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29010</th>\n",
       "      <td>2002</td>\n",
       "      <td>3310</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29011</th>\n",
       "      <td>2002</td>\n",
       "      <td>3311</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29013</th>\n",
       "      <td>2002</td>\n",
       "      <td>3313</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29015</th>\n",
       "      <td>2002</td>\n",
       "      <td>3315</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29016</th>\n",
       "      <td>2002</td>\n",
       "      <td>3316</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29018</th>\n",
       "      <td>2002</td>\n",
       "      <td>3318</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29019</th>\n",
       "      <td>2002</td>\n",
       "      <td>3319</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29020</th>\n",
       "      <td>2002</td>\n",
       "      <td>3320</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29021</th>\n",
       "      <td>2002</td>\n",
       "      <td>3321</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29022</th>\n",
       "      <td>2002</td>\n",
       "      <td>3322</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29024</th>\n",
       "      <td>2002</td>\n",
       "      <td>3401</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29026</th>\n",
       "      <td>2002</td>\n",
       "      <td>3403</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29029</th>\n",
       "      <td>2002</td>\n",
       "      <td>3501</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29030</th>\n",
       "      <td>2002</td>\n",
       "      <td>3503</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29034</th>\n",
       "      <td>2002</td>\n",
       "      <td>3600</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29035</th>\n",
       "      <td>2002</td>\n",
       "      <td>3601</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29037</th>\n",
       "      <td>2002</td>\n",
       "      <td>3603</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29038</th>\n",
       "      <td>2002</td>\n",
       "      <td>3605</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29039</th>\n",
       "      <td>2002</td>\n",
       "      <td>3607</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29041</th>\n",
       "      <td>2002</td>\n",
       "      <td>3610</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29042</th>\n",
       "      <td>2002</td>\n",
       "      <td>3611</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>309 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sub1  sub2  distance\n",
       "28830  2002  2002  0.000000\n",
       "29012  2002  3312  0.086777\n",
       "28999  2002  3207  0.129757\n",
       "29001  2002  3301  0.154026\n",
       "29007  2002  3307  0.179051\n",
       "29005  2002  3305  0.184566\n",
       "29017  2002  3317  0.205703\n",
       "28850  2002  2213  0.236648\n",
       "29006  2002  3306  0.238677\n",
       "28996  2002  3204  0.258080\n",
       "28898  2002  2701  0.265088\n",
       "28934  2002  2738  0.271287\n",
       "28915  2002  2719  0.272533\n",
       "28971  2002  2919  0.272610\n",
       "28994  2002  3202  0.276769\n",
       "28846  2002  2209  0.277370\n",
       "28995  2002  3203  0.279720\n",
       "29043  2002  3612  0.292031\n",
       "28963  2002  2910  0.299548\n",
       "28806  2002  1708  0.301755\n",
       "28935  2002  2739  0.304889\n",
       "28998  2002  3206  0.307462\n",
       "28992  2002  3200  0.307856\n",
       "28954  2002  2900  0.313989\n",
       "28962  2002  2909  0.314182\n",
       "29014  2002  3314  0.320662\n",
       "28966  2002  2913  0.328911\n",
       "28997  2002  3205  0.329097\n",
       "28913  2002  2717  0.330693\n",
       "28964  2002  2911  0.332313\n",
       "...     ...   ...       ...\n",
       "28976  2002  3001       NaN\n",
       "28984  2002  3103       NaN\n",
       "28990  2002  3109       NaN\n",
       "28991  2002  3110       NaN\n",
       "28993  2002  3201       NaN\n",
       "29000  2002  3300       NaN\n",
       "29002  2002  3302       NaN\n",
       "29003  2002  3303       NaN\n",
       "29009  2002  3309       NaN\n",
       "29010  2002  3310       NaN\n",
       "29011  2002  3311       NaN\n",
       "29013  2002  3313       NaN\n",
       "29015  2002  3315       NaN\n",
       "29016  2002  3316       NaN\n",
       "29018  2002  3318       NaN\n",
       "29019  2002  3319       NaN\n",
       "29020  2002  3320       NaN\n",
       "29021  2002  3321       NaN\n",
       "29022  2002  3322       NaN\n",
       "29024  2002  3401       NaN\n",
       "29026  2002  3403       NaN\n",
       "29029  2002  3501       NaN\n",
       "29030  2002  3503       NaN\n",
       "29034  2002  3600       NaN\n",
       "29035  2002  3601       NaN\n",
       "29037  2002  3603       NaN\n",
       "29038  2002  3605       NaN\n",
       "29039  2002  3607       NaN\n",
       "29041  2002  3610       NaN\n",
       "29042  2002  3611       NaN\n",
       "\n",
       "[309 rows x 3 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "econ_df = df1.loc[df1[\"sub1\"] == 2002]\n",
    "econ_df.sort_values(['distance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(\"dist_2013\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
